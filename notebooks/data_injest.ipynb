{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from zep_cloud.client import Zep\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    url=\"https://3973cdf9-4ba6-40b1-ae92-b2f952f82fb9.europe-west3-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=os.getenv(\"QDRANT_CLOUD_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zep_client = Zep(\n",
    "    api_key=os.environ.get('ZEP_API_KEY'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user if user not created\n",
    "\n",
    "zep_client.user.add(\n",
    "    email=\"test@email.com\",\n",
    "    first_name=\"Test\",\n",
    "    last_name=\"User\",\n",
    "    user_id=\"user_1\", # do not change the id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[]\n"
     ]
    }
   ],
   "source": [
    "print(client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcripts(data_dir):\n",
    "    transcripts = []\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as f:\n",
    "                transcripts.append(f.read())\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transcript(transcript):\n",
    "    title_match = re.search(r\"Title: (.+)\", transcript)\n",
    "    url_match = re.search(r\"URL Source: (.+)\", transcript)\n",
    "    content_match = re.search(r\"Markdown Content:(.+)\", transcript, re.DOTALL)\n",
    "\n",
    "    return {\n",
    "        \"title\": title_match.group(1) if title_match else None,\n",
    "        \"url\": url_match.group(1) if url_match else None,\n",
    "        \"content\": content_match.group(1).strip() if content_match else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def get_token_count_by_subtopic(subtopics):\n",
    "    token_counts = []\n",
    "    for subtopic in subtopics:\n",
    "        content = ' '.join(subtopic['content'])\n",
    "        tokens = tokenizer.encode(content)\n",
    "        token_counts.append({\n",
    "            'subtopic': subtopic['subtopic'],\n",
    "            'token_count': len(tokens)\n",
    "        })\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=500, min_tokens=300):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for token in tokens:\n",
    "        current_chunk.append(token)\n",
    "        # If the current chunk exceeds the max token limit\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "    # Handle the last chunk, ensure it meets the minimum size requirement\n",
    "    if current_chunk:\n",
    "        if len(current_chunk) < min_tokens and chunks:\n",
    "            # If the last chunk is smaller than the minimum, merge it with the previous chunk\n",
    "            chunks[-1].extend(current_chunk)\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_chunk_transcript_by_subtopic(data):\n",
    "    transcript = data[\"content\"]\n",
    "    # Regex to find subtopics (e.g., Introduction, Education)\n",
    "    subtopic_pattern = re.compile(r\"^(.*)\\n-+\\n\", re.MULTILINE)\n",
    "    # Regex to capture speaker dialogue (e.g., Destiny [(00:00:00)]...)\n",
    "    dialogue_pattern = re.compile(r\"(?P<speaker>\\w+)\\s\\[\\((?P<timestamp>\\d{2}:\\d{2}:\\d{2})\\)\\]\\((?P<url>https:\\/\\/youtube\\.com\\/watch\\?v=[^&]+&t=\\d+)\\)\\s(?P<text>.+)\")\n",
    "    \n",
    "    chunks = []\n",
    "\n",
    "    subtopics = subtopic_pattern.split(transcript)\n",
    "\n",
    "    for i in range(1, len(subtopics), 2):\n",
    "        subtopic = subtopics[i].strip()\n",
    "        # print(subtopic)\n",
    "\n",
    "        content_block = subtopics[i + 1] if i + 1 < len(subtopics) else \"\"\n",
    "\n",
    "        # update the current subtopic\n",
    "        current_subtopic = subtopic\n",
    "        \n",
    "        # Find all dialogues within this subtopic\n",
    "        dialogues = dialogue_pattern.findall(content_block)\n",
    "\n",
    "        # print(dialogues)\n",
    "\n",
    "        formatted_text = []\n",
    "        speakers = []\n",
    "        tstamp = None\n",
    "        for dialogue in dialogues:\n",
    "            speaker, timestamp, url, text = dialogue\n",
    "            while tstamp == None:\n",
    "                tstamp = f\"[({timestamp})]({url})\"\n",
    "\n",
    "            if speaker not in speakers:\n",
    "                speakers.append(speaker)\n",
    "\n",
    "            formatted_text.append(f\"{speaker}: {text} \\n\")\n",
    "        \n",
    "        # token count\n",
    "        tokens_enc = tokenizer.encode(' '.join(formatted_text))\n",
    "        tok_count = len(tokens_enc)\n",
    "\n",
    "        if tok_count > 500:\n",
    "            token_chunks = chunk_text(' '.join(formatted_text))\n",
    "            for chunk in token_chunks:\n",
    "                current_chunk = {\n",
    "                    \"subtopic\": subtopic,\n",
    "                    \"content\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"speakers\": speakers,\n",
    "                        \"dialogue_count\": len(chunk),\n",
    "                        \"title\": data[\"title\"],\n",
    "                        \"url\": data[\"url\"],\n",
    "                        \"timestamp\": tstamp\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(current_chunk)\n",
    "        else:\n",
    "            current_chunk = {\n",
    "                \"subtopic\": subtopic,\n",
    "                \"content\": formatted_text,\n",
    "                \"metadata\": {\n",
    "                    \"speakers\": speakers,\n",
    "                    \"dialogue_count\": len(formatted_text),\n",
    "                    \"title\": data[\"title\"],\n",
    "                    \"url\": data[\"url\"],\n",
    "                    \"timestamp\": tstamp\n",
    "                }\n",
    "            }\n",
    "            chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, VectorParams, Batch\n",
    "from qdrant_client import models\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.Client()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "VECTOR_SIZE = 1536  # Size of OpenAI's text-embedding-3-small model output\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get OpenAI embedding for the given text.\"\"\"\n",
    "\n",
    "    response = openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
    "    return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embe = get_embedding(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collections(collection_name: str, vector_size = 1536):\n",
    "    \"Create new collection in qdrant cloud\"\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=vector_size, \n",
    "            distance=models.Distance.COSINE,\n",
    "            hnsw_config=models.HnswConfigDiff(\n",
    "                m=16,\n",
    "                ef_construct=100,\n",
    "                full_scan_threshold=10000,\n",
    "                max_indexing_threads=0\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create indexes on metadata fields and full text\n",
    "    client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"subtopic\",\n",
    "        field_schema=models.PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"speakers\",\n",
    "        field_schema=models.PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"title\",\n",
    "        field_schema=models.PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"content\",\n",
    "        field_schema=models.PayloadSchemaType.TEXT\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = load_transcripts(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"podcasts\"\n",
    "create_collections(COLLECTION_NAME, VECTOR_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting embedding for []\n",
      "Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "for transcript in transcripts:\n",
    "    data = parse_transcript(transcript)\n",
    "    chunks = parse_and_chunk_transcript_by_subtopic(data)\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            vector = get_embedding(chunk[\"content\"])\n",
    "        except Exception as e:\n",
    "            # drop empty chunks\n",
    "            print(f\"Error getting embedding for {chunk['content']}\")\n",
    "            print(e)\n",
    "            continue\n",
    "        client.upsert(\n",
    "            collection_name=\"podcasts\",\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    vector=vector,\n",
    "                    payload={\n",
    "                        \"subtopic\": chunk[\"subtopic\"],\n",
    "                        \"speakers\": chunk[\"metadata\"][\"speakers\"],\n",
    "                        \"content\": chunk[\"content\"],\n",
    "                        \"title\": chunk[\"metadata\"][\"title\"],\n",
    "                        \"url\": chunk[\"metadata\"][\"url\"],\n",
    "                        \"timestamp\": chunk[\"metadata\"][\"timestamp\"]\n",
    "                    }\n",
    "                )\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "chunk = None\n",
    "for transcript in transcripts:\n",
    "    data = parse_transcript(transcript)\n",
    "    chunks = parse_and_chunk_transcript_by_subtopic(data)\n",
    "    cnt += len(chunks)\n",
    "    if cnt == 10:\n",
    "        chunk = chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
